{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "599e0de3-312a-46f4-9d6f-092a6195309c",
    "_uuid": "595d357e1d9a22ed268c8cf2d8c70ee35bd40036",
    "collapsed": true
   },
   "source": [
    "# <center> Beginner's guide: NN with multichannel input  in Keras</center>\n",
    "\n",
    "_Author: Kirill Vlasov_\n",
    "\n",
    "--------\n",
    "\n",
    "# Introduction\n",
    "In this article we will not discuss types of Neural Network. We will try to build network with multichannel input, because this case is so difficult for novice.  \n",
    "  \n",
    "\n",
    "__Plan:__\n",
    "- Explanation of model’s usefulness\n",
    "- How to develop a neural network with multichannel input in Keras.\n",
    "- Practice: using this approach in <a href=\"https://www.kaggle.com/c/donorschoose-application-screening\">DonorsChoose Competition</a>\n",
    "\n",
    "Let's start!\n",
    "\n",
    "# Explanation of model’s usefulness\n",
    "Imagine, we have a dataset of images and we need to solve the problem of classification. Probably, we will develop a convolutional neural network. What are you going to do, in order to supplement meta data (texts, some categorical features and etc.) in model?  \n",
    "Obviously, we need different types of NN for different types of data, e.g. RNN, CNN and etc. But NN with multichannel input allows to create ONE NN, which could merge all different types of needed NNs. It could divide different flows of calculation, and then merge them together inside one joint NN.\n",
    "  \n",
    "# How to develop a neural network with multichannel input in Keras.\n",
    "\n",
    "- Firts of all, we define the type of each data and choose apropriate type of NN for each type of data. \n",
    "- Then, we develop each NN.\n",
    "- By class _concatenate_ of module _layers.merge_ in _Keras_ we merge all outputs of these different NNs\n",
    "- Enjoy! :) \n",
    " \n",
    "__That's all!__\n",
    "  \n",
    "# Practice: using this approach in DonorsChoose Competition </a>\n",
    "\n",
    "  \n",
    "## 0. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "cc97888a-c5bd-40c4-94e4-59c6033ed958",
    "_uuid": "9272c277feec50a9db24191cf111288835c72c7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suryavamsi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.models import Model, Sequential \n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "33e27d2f-3ecc-4183-9d14-c55db3b6d9a0",
    "_uuid": "057b44ef33dd214a06acaeb685145cb7f8be60c2"
   },
   "source": [
    "## 1. Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a8c57673-e12f-4689-9278-7a808db474ae",
    "_uuid": "a7752e4c1a35456297fdfb0985f6791f039320f4"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', low_memory=False, index_col='id')\n",
    "test = pd.read_csv('test.csv', low_memory=False, index_col='id')\n",
    "\n",
    "res = pd.read_csv('resources.csv', low_memory=False, index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a78f046e-8a37-4dc0-a1de-fd534f9a9062",
    "_uuid": "3a8d4cac34750764a1dc5968ce0101f4c1b4c09d"
   },
   "source": [
    "### 1.1. Concatination of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "856e4260-feff-4b22-9acc-ecf5ffd0edcd",
    "_uuid": "3c2f2c133e52b334e62a844b43976e931efdb79a"
   },
   "outputs": [],
   "source": [
    "train['is_train'] = 1\n",
    "test['is_train'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "1c4c8df5-2c9a-4f79-aba9-116fef516f39",
    "_uuid": "106065a39cb35d134c349d6b4fac2eebc234ad74"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "928bc38f-fcdd-498e-b1d9-fd06ea0f9fcf",
    "_uuid": "38c37cf4c6ae504211011f640a18c6d25f016a90"
   },
   "source": [
    "### 1.2. Generate features from 'res.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "88159521-2536-49a5-930b-e0b1ebe8295a",
    "_uuid": "353a95e10f04b278980660ac1514b8cf532512c0"
   },
   "outputs": [],
   "source": [
    "sum_res = pd.pivot_table(res, index=res.index, aggfunc='sum', values=['price', 'quantity'])\n",
    "mean_res = pd.pivot_table(res, index=res.index, aggfunc='mean', values=['price', 'quantity'])\n",
    "median_res = pd.pivot_table(res, index=res.index, aggfunc='median', values=['price', 'quantity'])\n",
    "\n",
    "df = pd.merge(df, sum_res,left_index=True, right_index=True)\n",
    "df = pd.merge(df, mean_res,left_index=True, right_index=True, suffixes=('_sum', ''))\n",
    "df = pd.merge(df, median_res,left_index=True, right_index=True, suffixes=('_mean', '_median'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f3d1713-6742-4a0e-a2f5-f1d5b4c15721",
    "_uuid": "2178a8af306337d835d7d1aa8b7724b52042a6ae"
   },
   "source": [
    "### 1.3. Type of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "0e0ff6da-565f-441e-a06c-8fba9c0fd31a",
    "_uuid": "635bc0ec177b1d19c3a77fabb616afa2449e60fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['is_train', 'project_essay_1', 'project_essay_2', 'project_essay_3',\n",
       "       'project_essay_4', 'project_grade_category', 'project_is_approved',\n",
       "       'project_resource_summary', 'project_subject_categories',\n",
       "       'project_subject_subcategories', 'project_submitted_datetime',\n",
       "       'project_title', 'school_state', 'teacher_id',\n",
       "       'teacher_number_of_previously_posted_projects', 'teacher_prefix',\n",
       "       'price_sum', 'quantity_sum', 'price_mean', 'quantity_mean',\n",
       "       'price_median', 'quantity_median'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b75307a0-da36-4af9-a39a-f0d39fb7179d",
    "_uuid": "bef0b9bfcd754f3f78cd3ab8c3f77bad00732fa7"
   },
   "outputs": [],
   "source": [
    "cat_feature = ['school_state', 'teacher_prefix', \n",
    "               'project_subject_categories', 'project_subject_subcategories', 'project_grade_category']\n",
    "\n",
    "target = 'project_is_approved'\n",
    "\n",
    "text_feature = ['project_title', 'project_resource_summary', 'project_essay_1', 'project_essay_2', 'project_essay_3',\n",
    "       'project_essay_4' ]\n",
    "\n",
    "real_feature = ['teacher_number_of_previously_posted_projects', 'price_sum', 'quantity_sum', 'price_mean', 'quantity_mean',\n",
    "       'price_median', 'quantity_median' ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54b10802-63fc-4869-adc9-674ea43b69b1",
    "_uuid": "367294aa339660dd198857f56e1e165701bdfee0"
   },
   "source": [
    "### 1.4. Preprocessing of features \n",
    "__Categorical__  \n",
    "We may just facrorize features of this type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "563fd978-1c81-40c2-92c1-71a30a061aa7",
    "_uuid": "75c0fe1387ae9b6d2f372ad09009ad0f3103ee32"
   },
   "outputs": [],
   "source": [
    "for i in cat_feature:\n",
    "    df[i] = pd.factorize(df[i])[0]\n",
    "\n",
    "trn_cat = df[cat_feature].values[:182080]\n",
    "tst_cat = df[cat_feature].values[182080:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "efd60470-8f72-421c-873f-d29be83facdc",
    "_uuid": "c7ffc042ba7a88d21dd2ad697b687bb0d6c3ba99"
   },
   "source": [
    "__Real__  \n",
    "Don't forget about _Scalling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "fdf17bbb-d773-4c5f-9a1c-fd50def722bd",
    "_uuid": "2d3dfc9590de0f2b8983992159960849c3b16f88"
   },
   "outputs": [],
   "source": [
    "SS = StandardScaler()\n",
    "df_scale = SS.fit_transform(df[real_feature])\n",
    "\n",
    "trn_real = df_scale[:182080]\n",
    "tst_real = df_scale[182080:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9839b71c-2b78-4ad2-a987-c04654ba6772",
    "_uuid": "7b49562a1790f475009fa4dee228b75e1e6e30fd"
   },
   "source": [
    "__Text__  \n",
    "Processing of text data easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "9430c0fe-5f59-4604-bb86-1985b70c94f6",
    "_uuid": "bea34f69045c990d69a9c9816e662c067b179890"
   },
   "outputs": [],
   "source": [
    "df_text = df[text_feature].fillna(' ')\n",
    "df_text['full_text'] = ''\n",
    "for f in text_feature:\n",
    "    df_text['full_text'] = df_text['full_text'] + df_text[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "275a9433-3043-4698-a517-cf298396670e",
    "_uuid": "e1b73d863a32639dbdce40d81640f64b58f78f84"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def clean(text):\n",
    "    return re.sub('[!@#$:]', '', ' '.join(re.findall('\\w{3,}', str(text).lower())))\n",
    "\n",
    "def stem(text):\n",
    "    return ' '.join([stemmer.stem(w) for w in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "5c64c8f1-df6c-4f27-813a-52b10528de0d",
    "_uuid": "523e517e217e6fb433c12b7a3313d902d5da9602"
   },
   "outputs": [],
   "source": [
    "df_text['full_text'] = df_text['full_text'].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "3ece29f2-4a8d-4c12-8604-72d74869a42c",
    "_uuid": "6cbde2e2ab0f89d029228aa5ed32ffc5d10eb7b7"
   },
   "outputs": [],
   "source": [
    "#df_text['full_text'] = df_text['full_text'].apply(lambda x: stem(x)) - don't think about it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "4ee936d8-3a53-4aef-b0ef-4021ffae747f",
    "_uuid": "c331bf05cdd4d1eb5eb31ae1447be273523bcf7f"
   },
   "outputs": [],
   "source": [
    "max_words = 500 #more words for more accuracy\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_text['full_text'])\n",
    "\n",
    "trn_text = tokenizer.texts_to_matrix(df_text['full_text'][:182080], mode='binary')\n",
    "tst_text = tokenizer.texts_to_matrix(df_text['full_text'][182080:], mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06728fe9-6979-4eeb-bbb8-f8346f48fadf",
    "_uuid": "d894ea2bc04a362419cbfa2193da7c3905f7c89f"
   },
   "source": [
    "__Target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "268a4ea2-83b7-48c1-b428-330174e68e1b",
    "_uuid": "e9403c7e93377554d52fb306c3a3b44549d985cf"
   },
   "outputs": [],
   "source": [
    "y = df[target].values[:182080]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec0574e1-eb23-4a66-8cc0-2bf81850a89a",
    "_uuid": "5fce0b3a0161c67be738bb8701b00706d4c66c8b"
   },
   "source": [
    "## 2. Modeling! \n",
    "### 2.1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "ee567d4d-d6a9-48a5-b7cc-f3ec252ede99",
    "_uuid": "168f2eab4120a664441b60094d19710e5ee63c8e"
   },
   "outputs": [],
   "source": [
    "len_cat = trn_cat.shape[1]\n",
    "len_real = trn_real.shape[1]\n",
    "len_text = trn_text.shape[1]\n",
    "\n",
    "\n",
    "size_embedding = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b879dbc6-4cfa-4448-9891-ce693bb5aa1d",
    "_uuid": "4452f9287de9004d7114ef6fd7033805b4f31585"
   },
   "source": [
    "### 2.2. Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "29d6414d-eb95-4776-a236-ae18d1c99673",
    "_uuid": "c1a5975e61a427c836bb7d5a085b2a35349b3eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1536        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          2048        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 500, 36)      180000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          32896       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 493, 32)      9248        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           8256        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 493, 32)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           2080        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 32)           2080        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 246, 32)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           1056        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 32)           1056        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7872)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7936)         0           dense_5[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 200)          1587400     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 20)           4020        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            21          dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,872,849\n",
      "Trainable params: 1,872,849\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# categorical channel \n",
    "inputs1 = Input(shape=(len_cat,))\n",
    "dense_cat_1 = Dense(256, activation='relu')(inputs1)\n",
    "dense_cat_2 = Dense(128, activation='relu')(dense_cat_1)\n",
    "dense_cat_3 = Dense(64, activation='relu')(dense_cat_2)\n",
    "dense_cat_4 = Dense(32, activation='relu')(dense_cat_3)\n",
    "flat1 = Dense(32, activation='relu')(dense_cat_4)\n",
    "\n",
    "\n",
    "\n",
    "# real channel\n",
    "inputs2 = Input(shape=(len_real,))\n",
    "dense_real_1 = Dense(256, activation='relu')(inputs2)\n",
    "dense_real_2 = Dense(128, activation='relu')(dense_real_1)\n",
    "dense_real_3 = Dense(64, activation='relu')(dense_real_2)\n",
    "dense_real_4 = Dense(32, activation='relu')(dense_real_3)\n",
    "flat2 = Dense(32, activation='relu')(dense_real_4)\n",
    "\n",
    "\n",
    "# text chanel\n",
    "inputs3 = Input(shape=(len_text,))\n",
    "embedding3 = Embedding(size_embedding, 36)(inputs3)\n",
    "conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "drop3 = Dropout(0.1)(conv3)\n",
    "pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "flat3 = Flatten()(pool3)\n",
    "\n",
    "# merge\n",
    "merged = concatenate([flat1, flat2, flat3])\n",
    "\n",
    "# interpretation\n",
    "dense1 = Dense(200, activation='relu')(merged)\n",
    "dense2 = Dense(20, activation='relu')(dense1)\n",
    "outputs = Dense(1, activation='sigmoid')(dense2)\n",
    "model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "10f67cac-1a10-43fa-81b0-feeecb8cb5fd",
    "_uuid": "b5cc6714af53e6c5f78cab2a69b50d2a86031f7f"
   },
   "source": [
    "### 2.4. Metric  \n",
    "Thx Stackoverflow for realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "adcff888-b02a-4950-9fd0-44b34ff1f9d9",
    "_uuid": "9fcb51c32095d8704dd7c2d7e0f27ca9673926cc"
   },
   "outputs": [],
   "source": [
    "# AUC for a binary classifier\n",
    "def auc(y_true, y_pred):   \n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)    \n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)    \n",
    "    return TP/P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29f72816-b356-449f-89ca-47478f689292",
    "_uuid": "77a251488b0a929a3708ce4e6e7ce28695af07be"
   },
   "source": [
    "### 2.3. Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "98914160-7ad3-4089-8d8a-63c61b8629f4",
    "_uuid": "d112c121f144134718a9ff9ba7ca6d378b60f7d1"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c3e9af28-f616-4571-9c90-d1884ace0929",
    "_uuid": "145aca20c8c592a27acfcb8c5ec52ec74fc4e455"
   },
   "source": [
    "### 2.4. Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "2d2de78e-d028-4be2-954c-f5176b50f413",
    "_uuid": "11dd7b41e082eb26ae7fea7c54754f8eaf764434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 145664 samples, validate on 36416 samples\n",
      "Epoch 1/3\n",
      "145664/145664 [==============================] - 471s 3ms/step - loss: 0.4002 - acc: 0.8463 - auc: 0.6886 - val_loss: 0.3856 - val_acc: 0.8487 - val_auc: 0.7236\n",
      "Epoch 2/3\n",
      "145664/145664 [==============================] - 403s 3ms/step - loss: 0.3830 - acc: 0.8491 - auc: 0.7290 - val_loss: 0.3821 - val_acc: 0.8496 - val_auc: 0.7257\n",
      "Epoch 3/3\n",
      "145664/145664 [==============================] - 417s 3ms/step - loss: 0.3797 - acc: 0.8495 - auc: 0.7350 - val_loss: 0.3802 - val_acc: 0.8503 - val_auc: 0.7307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd7642ba7f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "model.fit([trn_cat, trn_real, trn_text], y, batch_size=batch_size, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0c2b1ec7-b282-42c1-a98f-04b88b11255b",
    "_uuid": "27ae23925b4c0497d82e126a36420fdb36428afd"
   },
   "source": [
    "### 2.5. Submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "bb557438-6ba5-4984-9b3d-97a65818fe9b",
    "_uuid": "2c8a061e2e58b58d428116b1dc32546f1c8b9280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78035/78035 [==============================] - 61s 778us/step\n"
     ]
    }
   ],
   "source": [
    "submit = model.predict([tst_cat, tst_real, tst_text], batch_size=batch_size,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "ce6e2624-7a24-4ec7-a574-5c83d0159143",
    "_uuid": "524270cd696ddec8bc35ff2a9f7969a25e84f4c3"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "77d66c87-782d-451f-b565-89239ac29000",
    "_uuid": "438d6f4c44de8979604ea0fbac435778c7f812de"
   },
   "outputs": [],
   "source": [
    "submission['project_is_approved'] = submit\n",
    "submission.to_csv('mi_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "169c47b1-4da9-42de-a503-b87612a831dc",
    "_uuid": "7a03f15dc624ab8135794e4bd4f4c9d5ad3d4acf"
   },
   "source": [
    "## 3.Comparison with non-multichannel type of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "951c8393-6e78-4144-b797-2a97ae43fe5e",
    "_uuid": "38a236616af3b36b4fb1e4c69c98abded0f3f8b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182080, 512)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_all = np.hstack((trn_cat, trn_real, trn_text))\n",
    "trn_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "e4c76f98-d1cb-4c3c-91eb-2ed6af686f06",
    "_uuid": "b48fac07b54a14929ed395751bc040ce9d5b58cb"
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(256, input_shape=(trn_all.shape[1],), activation='relu'))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "397e69af-8832-4cf6-9e75-67b6cbb49cfc",
    "_uuid": "904ecdbb81d04a095e4c8374fea9440461e4e31a"
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "1f7fc08e-ed92-44fc-88f3-0d761a9e145b",
    "_uuid": "b138b95229b6085c17fe4efc418c480452a7760c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 145664 samples, validate on 36416 samples\n",
      "Epoch 1/3\n",
      "145664/145664 [==============================] - 13s 86us/step - loss: 0.4539 - acc: 0.8379 - auc: 0.6243 - val_loss: 0.3927 - val_acc: 0.8481 - val_auc: 0.7031\n",
      "Epoch 2/3\n",
      "145664/145664 [==============================] - 7s 49us/step - loss: 0.3888 - acc: 0.8480 - auc: 0.7167 - val_loss: 0.3870 - val_acc: 0.8492 - val_auc: 0.7168\n",
      "Epoch 3/3\n",
      "145664/145664 [==============================] - 6s 44us/step - loss: 0.3806 - acc: 0.8490 - auc: 0.7329 - val_loss: 0.3872 - val_acc: 0.8496 - val_auc: 0.7144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd75bbf3d30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2000\n",
    "model2.fit(trn_all, y, batch_size=batch_size, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cef41503-0c38-4148-a295-2e59ff69df4d",
    "_uuid": "26e535947b6c6431e41f2e31477092a91220549e"
   },
   "source": [
    "# Conclusion\n",
    "Certainly, computing power of Kaggle's kernel doesn't allow to build more sophisticated models, but in practice we may experiment with NN with multichannel input to achieve better results. Finally, NN with multichannel input are more flexible and let you work with different types of data. \n",
    "\n",
    "\n",
    "\n",
    "# Links\n",
    "- <a href = \"https://keras.io\" > Keras Documentation </a>\n",
    "- <a href = \"https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/\" >How to Develop an N-gram Multichannel Convolutional Neural Network for Sentiment Analysis </a>\n",
    "- <a href = https://towardsdatascience.com/neural-network-architectures-156e5bad51ba> Neural Network Architectures </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ed9bb8f0-6f3a-45cc-85b1-a4da298f57c5",
    "_uuid": "b1073ab5948ef2d5ac0060e1b1f7d4303fec4893"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
